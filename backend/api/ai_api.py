from fastapi import APIRouter
from pydantic import BaseModel
from llama_cpp import Llama
import os
import json
from typing import Union

router = APIRouter(prefix="/ai", tags=["AI"])

# è¼‰å…¥æ¨¡å‹ï¼Œåªåšä¸€æ¬¡ï¼ˆå…¨åŸŸå–®ä¾‹ï¼‰
MODEL_PATH = 'ai_models/mistral-7b-instruct-v0.2.Q5_K_M.gguf'
llm = Llama(model_path=MODEL_PATH, n_ctx=2048)

class AskRequest(BaseModel):
    prompt: str

@router.post("/ask")
def ask_ai(req: AskRequest):
    output = llm(
        req.prompt,
        max_tokens=128,
        stop=["</s>"],
        echo=False,
        temperature=0.8,
    )
    answer = output["choices"][0]["text"].strip()
    return {"answer": answer}

class SummaryRequest(BaseModel):
    topic: str = None
    comments: list = None
    participants: list = None
    is_anonymous: bool = None
    discussion_goal: str = None
    duration: Union[str, int, None] = None

@router.post("/summary")
def summary_ai(req: SummaryRequest):
    if req.topic == "__from_file__":
        with open("meeting_log/test.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        req = SummaryRequest(**data)

    prompt = ""
    if req.topic:
        prompt += f"ä¸»é¡Œ: {req.topic}\n"
    if req.participants:
        prompt += f"åƒèˆ‡è€…: {', '.join(req.participants)}\n"
    if req.is_anonymous is not None:
        prompt += f"æ˜¯å¦åŒ¿å: {'æ˜¯' if req.is_anonymous else 'å¦'}\n"
    if req.discussion_goal:
        prompt += f"è¨è«–ç›®æ¨™: {req.discussion_goal}\n"

    prompt += "\nç•™è¨€èˆ‡ç¥¨æ•¸:\n"

    if req.comments:
        for c in req.comments:
            nickname = c.get("nickname", "åŒ¿å")
            content = c.get("content", "")
            likes = c.get("likes", 0)
            dislikes = c.get("dislikes", 0)
            t = c.get("time", "")
            prompt += f"- {nickname}ï¼š{content}ï¼ˆğŸ‘{likes}ã€ğŸ‘{dislikes}ï¼Œ{t}ï¼‰\n"

    prompt += """
                è«‹ç”¨æ¢åˆ—å¼å½™æ•´æœ¬æ¬¡è¨è«–çš„ä¸»è¦è§€é»èˆ‡çµè«–ï¼Œè‹¥æœ‰æ˜é¡¯æ­£åæ–¹æ„è¦‹è«‹åˆ†é–‹æ•´ç†ã€‚
                è«‹å‹™å¿…æ ¹æ“šä¸‹æ–¹æ¯ä¸€ç­†ç•™è¨€èˆ‡çµ±è¨ˆè³‡è¨Šï¼Œä¸è¦è‡ªè¡Œè‡†æ¸¬æˆ–ç”Ÿæˆä¸å­˜åœ¨çš„æ•¸å­—æˆ–åç¨±ã€‚
                è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼Œæ¯ä¸€é …éƒ½è¦æœ‰ä¸»é¡Œã€ä¸»æµæ„è¦‹ã€åˆ†æ­§é»ã€å¯èƒ½æ±ºè­°ï¼š
                ---
                1. <é‡é»ä¸»é¡Œ>
                - ä¸»æµæ„è¦‹ï¼š
                - åˆ†æ­§é»ï¼šï¼ˆè‹¥ç„¡å‰‡å¯«â€œç„¡â€ï¼‰
                - å¯èƒ½æ±ºè­°ï¼š
                ---
                æœ€å¾Œä»¥ã€Œç¸½çµã€å€å¡Šæ¢åˆ—æœ¬æ¬¡æœƒè­°å¾—åˆ°çš„æœ€é‡è¦å…±è­˜æˆ–AIå»ºè­°å¾ŒçºŒè¿½è¹¤äº‹é …ã€‚
            """

    output = llm(
        prompt,
        max_tokens=1024,
        stop=["</s>"],
        echo=False,
        temperature=0.8,
    )
    summary_text = output["choices"][0]["text"].strip()
    return {"summary": summary_text}
