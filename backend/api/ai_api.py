from fastapi import APIRouter
from pydantic import BaseModel
from llama_cpp import Llama
import os
import json
from typing import Union, List
from .participants_api import ROOMS, topics, votes

router = APIRouter(prefix="/ai", tags=["AI"])

# è¼‰å…¥æ¨¡å‹ï¼Œåªåšä¸€æ¬¡ï¼ˆå…¨åŸŸå–®ä¾‹ï¼‰
MODEL_PATH = 'ai_models/mistral-7b-instruct-v0.2.Q5_K_M.gguf'
llm = Llama(model_path=MODEL_PATH, n_ctx=2048)

class AskRequest(BaseModel):
    prompt: str

@router.post("/ask")
def ask_ai(req: AskRequest):
    output = llm(
        req.prompt,
        max_tokens=128,
        stop=["</s>"],
        echo=False,
        temperature=0.8,
    )
    answer = output["choices"][0]["text"].strip()
    return {"answer": answer}

# å®šç¾©ç”¨æ–¼ AI ç¸½çµçš„è«‹æ±‚æ¨¡å‹
class SummaryRequest(BaseModel):
    room: str  # æœƒè­°å®¤ä»£ç¢¼
    topic: str # è¦ç¸½çµçš„ä¸»é¡Œ

# --- é‡å¯« summary_ai å‡½å¼ ---
@router.post("/summary")
def summary_ai(req: SummaryRequest):
    """
    å°æŒ‡å®šæœƒè­°å®¤çš„ç‰¹å®šä¸»é¡Œé€²è¡Œ AI ç¸½çµ

    [POST] /ai/summary

    åƒæ•¸ï¼š
    - room (str): æœƒè­°å®¤ä»£ç¢¼
    - topic (str): è¦é€²è¡Œç¸½çµçš„ä¸»é¡Œåç¨±

    å›å‚³ï¼š
    - summary (str): AI ç”Ÿæˆçš„ç¸½çµæ–‡å­—
    """
    # æª¢æŸ¥æœƒè­°å®¤æ˜¯å¦å­˜åœ¨
    if req.room not in ROOMS:
        return {"summary": "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æœƒè­°å®¤ã€‚"}

    # çµ„åˆä¸»é¡Œ ID ä¸¦æª¢æŸ¥ä¸»é¡Œæ˜¯å¦å­˜åœ¨
    topic_id = f"{req.room}_{req.topic}"
    if topic_id not in topics:
        return {"summary": "éŒ¯èª¤ï¼šåœ¨è©²æœƒè­°å®¤ä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„ä¸»é¡Œã€‚"}

    room_data = ROOMS[req.room]
    topic_data = topics[topic_id]
    
    # --- é–‹å§‹å»ºç«‹ Prompt ---
    prompt = f"ä¸»é¡Œ: {req.topic}\n"

    # å–å¾—åƒèˆ‡è€…åˆ—è¡¨
    participants = [p.get("nickname", "åŒ¿å") for p in room_data.get("participants_list", [])]
    if participants:
        prompt += f"åƒèˆ‡è€…: {', '.join(participants)}\n"

    prompt += "\nç•™è¨€èˆ‡ç¥¨æ•¸:\n"

    # å–å¾—è©²ä¸»é¡Œçš„æ‰€æœ‰ç•™è¨€èˆ‡å…¶å°æ‡‰çš„ç¥¨æ•¸
    comments_for_prompt = []
    if "comments" in topic_data:
        for c in topic_data["comments"]:
            comment_id = c.get("id")
            nickname = c.get("nickname", "åŒ¿å")
            content = c.get("content", "")
            
            # å¾ votes å­—å…¸ä¸­å–å¾—ç¥¨æ•¸
            good_votes = len(votes.get(comment_id, {}).get("good", []))
            bad_votes = len(votes.get(comment_id, {}).get("bad", []))

            comments_for_prompt.append(
                f"- {nickname}ï¼š{content}ï¼ˆğŸ‘{good_votes}ã€ğŸ‘{bad_votes}ï¼‰"
            )

    if not comments_for_prompt:
        prompt += "ç›®å‰é€™å€‹ä¸»é¡Œé‚„æ²’æœ‰ä»»ä½•ç•™è¨€ã€‚\n"
    else:
        prompt += "\n".join(comments_for_prompt)

    # åŠ ä¸Šå›ºå®šçš„æŒ‡ä»¤æ¨¡æ¿
    prompt += """

                    ä½ çš„ä»»å‹™æ˜¯æ“”ä»»ä¸€å€‹å°ˆæ¥­çš„æœƒè­°è¨˜éŒ„å“¡ã€‚
                    ä½ å¿…é ˆåš´æ ¼æ ¹æ“šä¸Šæ–¹æä¾›çš„ã€Œç•™è¨€èˆ‡ç¥¨æ•¸ã€è³‡è¨Šï¼Œé€²è¡Œæ¢åˆ—å¼å½™æ•´ã€‚
                    ç¦æ­¢è‡†æ¸¬æˆ–ç”Ÿæˆä»»ä½•æœªåœ¨è³‡æ–™ä¸­å‡ºç¾çš„æ•¸å­—ã€åç¨±æˆ–è§€é»ã€‚
                    ä½ çš„å›ç­”å…§å®¹ï¼Œåªèƒ½åŒ…å«å½™æ•´å¾Œçš„çµæœï¼Œç¦æ­¢åŠ å…¥ä»»ä½•é–‹å ´ç™½ã€å•å€™èªæˆ–çµå°¾çš„å…è²¬è²æ˜ã€‚

                    è«‹ç›´æ¥ä»¥ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼Œä¸¦å°‡çœŸå¯¦çš„å…§å®¹å¡«å…¥ï¼š
                    ---
                    1. [ç¬¬ä¸€å€‹é‡é»ä¸»é¡Œ]
                    - ä¸»æµæ„è¦‹ï¼š
                    - åˆ†æ­§é»ï¼šï¼ˆè‹¥ç„¡å‰‡å¯«â€œç„¡â€ï¼‰
                    - å¯èƒ½æ±ºè­°ï¼š
                    ---
                    2. [ç¬¬äºŒå€‹é‡é»ä¸»é¡Œ]
                    - ä¸»æµæ„è¦‹ï¼š
                    - åˆ†æ­§é»ï¼šï¼ˆè‹¥ç„¡å‰‡å¯«â€œç„¡â€ï¼‰
                    - å¯èƒ½æ±ºè­°ï¼š
                    ---
                    ç¸½çµï¼š
                    [æ­¤è™•æ¢åˆ—æœƒè­°çš„æœ€é‡è¦å…±è­˜æˆ–å¾ŒçºŒè¿½è¹¤äº‹é …]
                """

    # å‘¼å« AI æ¨¡å‹
    output = llm(
        prompt,
        max_tokens=1024,
        stop=["</s>"],
        echo=False,
        temperature=0.8,
    )
    
    summary_text = output["choices"][0]["text"].strip()
    return {"summary": summary_text}