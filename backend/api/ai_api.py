from fastapi import APIRouter
from pydantic import BaseModel
from llama_cpp import Llama
import random, string, time, json, re
from typing import Union, List
from .participants_api import ROOMS, topics, votes

router = APIRouter(prefix="/ai", tags=["AI"])

# è¼‰å…¥æ¨¡å‹ï¼Œåªåšä¸€æ¬¡ï¼ˆå…¨åŸŸå–®ä¾‹ï¼‰
MODEL_PATH = 'ai_models/mistral-7b-instruct-v0.2.Q5_K_M.gguf'
llm = Llama(model_path=MODEL_PATH, n_ctx=2048)

class AskRequest(BaseModel):
    prompt: str

@router.post("/ask")
def ask_ai(req: AskRequest):
    output = llm(
        req.prompt,
        max_tokens=128,
        stop=["</s>"],
        echo=False,
        temperature=0.8,
    )
    answer = output["choices"][0]["text"].strip()
    return {"answer": answer}

# å®šç¾©ç”¨æ–¼ AI ç¸½çµçš„è«‹æ±‚æ¨¡å‹
class SummaryRequest(BaseModel):
    room: str  # æœƒè­°å®¤ä»£ç¢¼
    topic: str # è¦ç¸½çµçš„ä¸»é¡Œ

# --- é‡å¯« summary_ai å‡½å¼ ---
@router.post("/summary")
def summary_ai(req: SummaryRequest):
    """
    å°æŒ‡å®šæœƒè­°å®¤çš„ç‰¹å®šä¸»é¡Œé€²è¡Œ AI ç¸½çµ

    [POST] /ai/summary

    åƒæ•¸ï¼š
    - room (str): æœƒè­°å®¤ä»£ç¢¼
    - topic (str): è¦é€²è¡Œç¸½çµçš„ä¸»é¡Œåç¨±

    å›å‚³ï¼š
    - summary (str): AI ç”Ÿæˆçš„ç¸½çµæ–‡å­—
    """
    # æª¢æŸ¥æœƒè­°å®¤æ˜¯å¦å­˜åœ¨
    if req.room not in ROOMS:
        return {"summary": "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æœƒè­°å®¤ã€‚"}

    # çµ„åˆä¸»é¡Œ ID ä¸¦æª¢æŸ¥ä¸»é¡Œæ˜¯å¦å­˜åœ¨
    topic_id = f"{req.room}_{req.topic}"
    if topic_id not in topics:
        return {"summary": "éŒ¯èª¤ï¼šåœ¨è©²æœƒè­°å®¤ä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„ä¸»é¡Œã€‚"}

    room_data = ROOMS[req.room]
    topic_data = topics[topic_id]
    
    # --- é–‹å§‹å»ºç«‹ Prompt ---
    prompt = f"ä¸»é¡Œ: {req.topic}\n"

    # å–å¾—åƒèˆ‡è€…åˆ—è¡¨
    participants = [p.get("nickname", "åŒ¿å") for p in room_data.get("participants_list", [])]
    if participants:
        prompt += f"åƒèˆ‡è€…: {', '.join(participants)}\n"

    prompt += "\nç•™è¨€èˆ‡ç¥¨æ•¸:\n"

    # å–å¾—è©²ä¸»é¡Œçš„æ‰€æœ‰ç•™è¨€èˆ‡å…¶å°æ‡‰çš„ç¥¨æ•¸
    comments_for_prompt = []
    if "comments" in topic_data:
        for c in topic_data["comments"]:
            comment_id = c.get("id")
            nickname = c.get("nickname", "åŒ¿å")
            content = c.get("content", "")
            
            # å¾ votes å­—å…¸ä¸­å–å¾—ç¥¨æ•¸
            good_votes = len(votes.get(comment_id, {}).get("good", []))
            bad_votes = len(votes.get(comment_id, {}).get("bad", []))

            comments_for_prompt.append(
                f"- {nickname}ï¼š{content}ï¼ˆğŸ‘{good_votes}ã€ğŸ‘{bad_votes}ï¼‰"
            )

    if not comments_for_prompt:
        prompt += "ç›®å‰é€™å€‹ä¸»é¡Œé‚„æ²’æœ‰ä»»ä½•ç•™è¨€ã€‚\n"
    else:
        prompt += "\n".join(comments_for_prompt)

    # åŠ ä¸Šå›ºå®šçš„æŒ‡ä»¤æ¨¡æ¿
    prompt += """

                    ä½ çš„ä»»å‹™æ˜¯æ“”ä»»ä¸€å€‹å°ˆæ¥­çš„æœƒè­°è¨˜éŒ„å“¡ã€‚
                    ä½ å¿…é ˆåš´æ ¼æ ¹æ“šä¸Šæ–¹æä¾›çš„ã€Œç•™è¨€èˆ‡ç¥¨æ•¸ã€è³‡è¨Šï¼Œé€²è¡Œæ¢åˆ—å¼å½™æ•´ã€‚
                    ç¦æ­¢è‡†æ¸¬æˆ–ç”Ÿæˆä»»ä½•æœªåœ¨è³‡æ–™ä¸­å‡ºç¾çš„æ•¸å­—ã€åç¨±æˆ–è§€é»ã€‚
                    ä½ çš„å›ç­”å…§å®¹ï¼Œåªèƒ½åŒ…å«å½™æ•´å¾Œçš„çµæœï¼Œç¦æ­¢åŠ å…¥ä»»ä½•é–‹å ´ç™½ã€å•å€™èªæˆ–çµå°¾çš„å…è²¬è²æ˜ã€‚

                    è«‹ç›´æ¥ä»¥ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼Œä¸¦å°‡çœŸå¯¦çš„å…§å®¹å¡«å…¥ï¼š
                    ---
                    1. [ç¬¬ä¸€å€‹é‡é»ä¸»é¡Œ]
                    - ä¸»æµæ„è¦‹ï¼š
                    - åˆ†æ­§é»ï¼šï¼ˆè‹¥ç„¡å‰‡å¯«â€œç„¡â€ï¼‰
                    - å¯èƒ½æ±ºè­°ï¼š
                    ---
                    2. [ç¬¬äºŒå€‹é‡é»ä¸»é¡Œ]
                    - ä¸»æµæ„è¦‹ï¼š
                    - åˆ†æ­§é»ï¼šï¼ˆè‹¥ç„¡å‰‡å¯«â€œç„¡â€ï¼‰
                    - å¯èƒ½æ±ºè­°ï¼š
                    ---
                    ç¸½çµï¼š
                    [æ­¤è™•æ¢åˆ—æœƒè­°çš„æœ€é‡è¦å…±è­˜æˆ–å¾ŒçºŒè¿½è¹¤äº‹é …]
                """

    # å‘¼å« AI æ¨¡å‹
    output = llm(
        prompt,
        max_tokens=1024,
        stop=["</s>"],
        echo=False,
        temperature=0.8,
    )
    
    summary_text = output["choices"][0]["text"].strip()
    return {"summary": summary_text}

def _generate_topics_from_title(meeting_title: str, topic_count: int, llm_instance: Llama) -> List[str]:
    """
    æ ¹æ“šæœƒè­°æ¨™é¡Œç”Ÿæˆä¸»é¡Œçš„æ ¸å¿ƒé‚è¼¯ã€‚
    é€™æ˜¯ä¸€å€‹å…§éƒ¨å‡½å¼ï¼Œæ—¨åœ¨è¢«å…¶ä»– API ç«¯é»èª¿ç”¨ã€‚
    """
    topic_count = max(1, min(10, topic_count))
    meeting_title = meeting_title.strip()

    if not meeting_title:
        return ["éŒ¯èª¤ï¼šæœƒè­°åç¨±ä¸å¯ç‚ºç©ºã€‚"]

    prompt = f"""
        ä½ çš„ä»»å‹™æ˜¯ä¸€ä½å°ˆæ¥­ä¸”é«˜æ•ˆçš„æœƒè­°ä¸»æŒäººã€‚
        è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€Œæœƒè­°åç¨±ã€ï¼Œç‚ºé€™æ¬¡æœƒè­°è…¦åŠ›æ¿€ç›ªå‡º {topic_count} å€‹æœ€é—œéµã€æœ€ç›¸é—œçš„è¨è«–è­°ç¨‹ä¸»é¡Œã€‚

        æœƒè­°åç¨±ï¼š"{meeting_title}"

        ä½ çš„å›è¦†å¿…é ˆæ˜¯ä¸€å€‹å–®å±¤çš„ JSON æ ¼å¼é™£åˆ— (a flat JSON array)ï¼Œé™£åˆ—ä¸­åªåŒ…å«ç°¡æ½”ä¸”éç©ºçš„ä¸»é¡Œå­—ä¸²ã€‚
        ç¦æ­¢åœ¨é™£åˆ—ä¸­åŒ…å«ä»»ä½•ç©ºå­—ä¸² ("")ã€å·¢ç‹€é™£åˆ—æˆ–å­—ä¸²åŒ–çš„ JSONã€‚
        è«‹ä¸è¦åŒ…å«ä»»ä½•æ•¸å­—ç·¨è™Ÿã€ç ´æŠ˜è™Ÿã€æˆ–ä»»ä½•å…¶ä»–çš„é–‹å ´ç™½èˆ‡è§£é‡‹ã€‚

        æ­£ç¢ºç¯„ä¾‹ï¼š
        ["å›é¡§ç¬¬äºŒå­£éŠ·å”®æ•¸æ“š", "è¨è«–æ–°åŠŸèƒ½å„ªå…ˆç´š", "è¨­å®šç¬¬ä¸‰å­£KPI"]

        éŒ¯èª¤ç¯„ä¾‹ï¼š
        ["", "ä¸»é¡ŒA", "[\"ä¸»é¡ŒB\", \"ä¸»é¡ŒC\"]"]
    """

    try:
        output = llm_instance(
            prompt,
            max_tokens=512,
            stop=["</s>"],
            echo=False,
            temperature=0.7,
        )
        
        raw_text = output["choices"][0]["text"].strip()

        # --- 4. è§£æ AI çš„å›è¦† (v4 çµ‚æ¥µç‰ˆ) ---
        
        # éè¿´å‡½å¼ï¼Œç”¨æ–¼æ”¤å¹³æ‰€æœ‰å¯èƒ½çš„å·¢ç‹€çµæ§‹
        def flatten_and_clean_topics(items):
            if not isinstance(items, list):
                return []
            
            flat_list = []
            for item in items:
                # å¦‚æœé …ç›®æ˜¯åˆ—è¡¨ï¼Œéè¿´æ”¤å¹³
                if isinstance(item, list):
                    flat_list.extend(flatten_and_clean_topics(item))
                # å¦‚æœé …ç›®æ˜¯å­—ä¸²
                elif isinstance(item, str):
                    # å»é™¤é ­å°¾ç©ºç™½
                    item = item.strip()
                    # å˜—è©¦å°‡å…¶è¦–ç‚º JSON é€²è¡Œè§£æ
                    if item.startswith('[') and item.endswith(']'):
                        try:
                            nested_list = json.loads(item)
                            flat_list.extend(flatten_and_clean_topics(nested_list))
                        except json.JSONDecodeError:
                            # è§£æå¤±æ•—ï¼Œç•¶ä½œæ™®é€šå­—ä¸²ï¼Œä½†éæ¿¾ç©ºå€¼
                            if item:
                                flat_list.append(item)
                    # å¦‚æœæ˜¯æ™®é€šå­—ä¸²ï¼Œéæ¿¾ç©ºå€¼
                    elif item:
                        flat_list.append(item)
            return flat_list

        try:
            # æ­¥é©Ÿ 1: ç§»é™¤æ½›åœ¨çš„ Markdown ç¨‹å¼ç¢¼å€å¡Šæ¨™ç±¤
            cleaned_text = re.sub(r'```(json)?\s*', '', raw_text)
            cleaned_text = cleaned_text.strip('`').strip()

            # æ­¥é©Ÿ 2: æ‰¾åˆ°æœ€å¤–å±¤çš„ []
            start_index = cleaned_text.find('[')
            end_index = cleaned_text.rfind(']') + 1
            
            if start_index != -1 and end_index != 0:
                json_str = cleaned_text[start_index:end_index]
                initial_topics = json.loads(json_str)
            else:
                # å¦‚æœæ‰¾ä¸åˆ° JSON çµæ§‹ï¼Œç›´æ¥æŒ‰è¡Œåˆ†å‰²
                raise ValueError("æ‰¾ä¸åˆ° JSON é™£åˆ—çµæ§‹")

            # æ­¥é©Ÿ 3: ä½¿ç”¨éè¿´å‡½å¼é€²è¡Œå¾¹åº•çš„æ”¤å¹³èˆ‡æ¸…ç†
            generated_topics = flatten_and_clean_topics(initial_topics)

        except (json.JSONDecodeError, ValueError):
            # æ­¥é©Ÿ 4: å¦‚æœ JSON è§£æå¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ (æŒ‰è¡Œåˆ†å‰²)
            cleaned_text = re.sub(r'```(json)?\s*', '', raw_text)
            cleaned_text = cleaned_text.strip('`').strip()
            generated_topics = [
                line.strip().lstrip('-*').lstrip('123456789.').strip()
                for line in cleaned_text.split('\n')
                if line.strip() and line.strip() not in ['[', ']']
            ]
        
        # --- 5. æœ€å¾Œçš„æ¸…ç†èˆ‡æ•¸é‡æ§åˆ¶ ---
        final_topics = [topic for topic in generated_topics if topic]
        return final_topics[:topic_count]

    except Exception as e:
        print(f"Error calling LLM for topic generation: {e}")
        return [f"æŠ±æ­‰ï¼ŒAI æœå‹™æš«æ™‚ç„¡æ³•é€£ç·šï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"]


class GenerateTopicsRequest(BaseModel):
    """ç”¨æ–¼ AI ç”Ÿæˆä¸»é¡Œè«‹æ±‚çš„æ¨¡å‹"""
    meeting_title: str
    topic_count: int

@router.post("/generate_topics")
def generate_ai_topics(req: GenerateTopicsRequest):
    """
    æ ¹æ“šæœƒè­°åç¨±å’ŒæŒ‡å®šæ•¸é‡ï¼Œä½¿ç”¨ AI ç”Ÿæˆè­°ç¨‹ä¸»é¡Œã€‚

    [POST] /ai/generate_topics

    åƒæ•¸ï¼š
    - meeting_title (str): æœƒè­°çš„æ¨™é¡Œæˆ–ä¸»è¦ç›®çš„ã€‚
    - topic_count (int): å¸Œæœ›ç”Ÿæˆçš„ä¸»é¡Œæ•¸é‡ã€‚

    å›å‚³ï¼š
    - topics (list[str]): ä¸€å€‹åŒ…å« AI ç”Ÿæˆçš„ä¸»é¡Œå­—ä¸²çš„åˆ—è¡¨ã€‚
    """
    # --- 1. åŸºæœ¬é©—è­‰ ---
    # ç¢ºä¿ä¸»é¡Œæ•¸é‡åœ¨ä¸€å€‹åˆç†çš„ç¯„åœå…§
    topic_count = max(1, min(10, req.topic_count)) 
    meeting_title = req.meeting_title.strip()

    if not meeting_title:
        return {"topics": ["éŒ¯èª¤ï¼šæœƒè­°åç¨±ä¸å¯ç‚ºç©ºã€‚"]}

    # --- 2. è¨­è¨ˆ Prompt (æŒ‡ä»¤) ---
    # é€™æ˜¯èˆ‡ AI æºé€šçš„é—œéµï¼Œæˆ‘å€‘çµ¦äºˆå®ƒè§’è‰²ã€ä»»å‹™å’Œæ˜ç¢ºçš„è¼¸å‡ºæ ¼å¼è¦æ±‚ã€‚
    prompt = f"""
        ä½ çš„ä»»å‹™æ˜¯ä¸€ä½å°ˆæ¥­ä¸”é«˜æ•ˆçš„æœƒè­°ä¸»æŒäººã€‚
        è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„ã€Œæœƒè­°åç¨±ã€ï¼Œç‚ºé€™æ¬¡æœƒè­°è…¦åŠ›æ¿€ç›ªå‡º {topic_count} å€‹æœ€é—œéµã€æœ€ç›¸é—œçš„è¨è«–è­°ç¨‹ä¸»é¡Œï¼Œä¸¦ä½¿ç”¨ç¹é«”ä¸­æ–‡å›è¦†ï¼

        æœƒè­°åç¨±ï¼š"{meeting_title}"

        ä½ çš„å›è¦†å¿…é ˆæ˜¯ä¸€å€‹ JSON æ ¼å¼çš„é™£åˆ— (array)ï¼Œé™£åˆ—ä¸­åªåŒ…å«ä¸»é¡Œçš„å­—ä¸²ã€‚
        è«‹ä¸è¦åŒ…å«ä»»ä½•æ•¸å­—ç·¨è™Ÿã€ç ´æŠ˜è™Ÿã€æˆ–ä»»ä½•å…¶ä»–çš„é–‹å ´ç™½èˆ‡è§£é‡‹ã€‚

        ä¾‹å¦‚ï¼Œå¦‚æœæœƒè­°åç¨±æ˜¯ã€Œ2025å¹´ç¬¬ä¸‰å­£ç”¢å“é–‹ç™¼ç­–ç•¥æœƒè­°ã€ï¼Œä¸¦ä¸”ä¸»é¡Œæ•¸é‡æ˜¯3çš„è©±ï¼Œä½ æ‡‰è©²å›å‚³ï¼š
        ["å›é¡§ç¬¬äºŒå­£éŠ·å”®æ•¸æ“šèˆ‡å®¢æˆ¶å›é¥‹", "è¨è«–æ–°åŠŸèƒ½å„ªå…ˆç´šèˆ‡é–‹ç™¼æ™‚ç¨‹", "è¨­å®šç¬¬ä¸‰å­£çš„é—œéµç¸¾æ•ˆæŒ‡æ¨™ (KPI)"]
    """

    # --- 3. å‘¼å«å¤§å‹èªè¨€æ¨¡å‹ (LLM) ---
    # å‡è¨­æ‚¨æœ‰ä¸€å€‹åç‚º llm çš„å‡½å¼ä¾†å‘¼å« AI æ¨¡å‹
    # é€™è£¡çš„åƒæ•¸å¯ä»¥æ ¹æ“šæ‚¨çš„æ¨¡å‹é€²è¡Œå¾®èª¿
    try:
        output = llm(
            prompt,
            max_tokens=512,  # ç”¢ç”Ÿçš„ token æ•¸é‡å¯ä»¥å°‘ä¸€äº›ï¼Œå› ç‚ºåªæ˜¯ä¸»é¡Œåˆ—è¡¨
            stop=["</s>"],
            echo=False,
            temperature=0.7, # æº«åº¦å¯ä»¥ç¨å¾®ä½ä¸€é»ï¼Œè®“ä¸»é¡Œæ›´èšç„¦
        )
        
        raw_text = output["choices"][0]["text"].strip()

        # --- 4. è§£æ AI çš„å›è¦† ---
        # æˆ‘å€‘å„ªå…ˆå˜—è©¦å°‡å›è¦†ç›´æ¥è§£æç‚º JSON
        try:
            # æ‰¾åˆ° JSON é™£åˆ—çš„é–‹å§‹å’ŒçµæŸä½ç½®ï¼Œä»¥æ‡‰å° AI å¯èƒ½åŠ å…¥å¤šé¤˜æ–‡å­—çš„æƒ…æ³
            start_index = raw_text.find('[')
            end_index = raw_text.rfind(']') + 1
            if start_index != -1 and end_index != 0:
                json_str = raw_text[start_index:end_index]
                generated_topics = json.loads(json_str)
                # ç¢ºä¿çµæœæ˜¯ä¸€å€‹åˆ—è¡¨
                if not isinstance(generated_topics, list):
                    raise ValueError("AI å›å‚³çš„ä¸æ˜¯ä¸€å€‹åˆ—è¡¨")
            else:
                 raise ValueError("åœ¨ AI å›æ‡‰ä¸­æ‰¾ä¸åˆ° JSON é™£åˆ—")

        except (json.JSONDecodeError, ValueError):
            # å¦‚æœ JSON è§£æå¤±æ•—ï¼Œæˆ‘å€‘é€€ä¸€æ­¥ï¼Œå˜—è©¦æŒ‰è¡Œåˆ†å‰²ä½œç‚ºå‚™ç”¨æ–¹æ¡ˆ
            # é€™èƒ½æ‡‰å° AI æœªå®Œå…¨éµå¾ªæ ¼å¼è¦æ±‚çš„æƒ…æ³
            generated_topics = [
                line.strip().lstrip('-').lstrip('*').lstrip('123456789.').strip() 
                for line in raw_text.split('\n') 
                if line.strip()
            ]
            # åªå–å›æˆ‘å€‘éœ€è¦çš„æ•¸é‡
            generated_topics = generated_topics[:topic_count]

        return {"topics": generated_topics}

    except Exception as e:
        # è™•ç†å‘¼å« AI æ™‚å¯èƒ½ç™¼ç”Ÿçš„ä»»ä½•éŒ¯èª¤
        print(f"Error calling LLM for topic generation: {e}")
        return {"topics": [f"æŠ±æ­‰ï¼ŒAI æœå‹™æš«æ™‚ç„¡æ³•é€£ç·šï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"]}

# æ–°å¢çš„è«‹æ±‚æ¨¡å‹ï¼Œç”¨æ–¼ generate_single_topic ç«¯é»
class GenerateSingleTopicRequest(BaseModel):
    room: str
    custom_prompt: str

@router.post("/generate_single_topic")
def generate_single_topic(req: GenerateSingleTopicRequest):
    """
    æ ¹æ“šæœƒè­°å®¤å’Œè‡ªè¨‚æç¤ºï¼Œä½¿ç”¨ AI ç”Ÿæˆå–®ä¸€è­°ç¨‹ä¸»é¡Œã€‚

    [POST] /ai/generate_single_topic

    åƒæ•¸ï¼š
    - room (str): æœƒè­°å®¤ä»£ç¢¼
    - custom_prompt (str): è‡ªè¨‚çš„æç¤ºèªå¥ï¼Œç”¨æ–¼å¼•å° AI ç”Ÿæˆä¸»é¡Œ

    å›å‚³ï¼š
    - topic (str): AI ç”Ÿæˆçš„ä¸»é¡Œå­—ä¸²
    """
    # æª¢æŸ¥æœƒè­°å®¤æ˜¯å¦å­˜åœ¨
    if req.room not in ROOMS:
        return {"topic": "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æœƒè­°å®¤ã€‚"}

    room_data = ROOMS[req.room]

    # --- é–‹å§‹å»ºç«‹ Prompt ---
    prompt = f"æœƒè­°å®¤: {req.room}\n"

    # å–å¾—åƒèˆ‡è€…åˆ—è¡¨
    participants = [p.get("nickname", "åŒ¿å") for p in room_data.get("participants_list", [])]
    if participants:
        prompt += f"åƒèˆ‡è€…: {', '.join(participants)}\n"

    prompt += "\nè«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šï¼Œç”Ÿæˆä¸€å€‹ç²¾ç°¡ä¸”å…·é«”çš„è­°ç¨‹ä¸»é¡Œã€‚"

    # åŠ ä¸Šä½¿ç”¨è€…è‡ªè¨‚çš„æç¤º
    prompt += f"\n\nè‡ªè¨‚æç¤º: {req.custom_prompt}"

    # å‘¼å« AI æ¨¡å‹
    output = llm(
        prompt,
        max_tokens=64,
        stop=["</s>"],
        echo=False,
        temperature=0.8,
    )
    
    topic = output["choices"][0]["text"].strip()
    return {"topic": topic}