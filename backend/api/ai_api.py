from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import json, random, time
from typing import List, Optional
from .participants_api import ROOMS, topics, votes
from .ai_config import ai_config
from .ai_client import ai_client
from .ai_prompts import prompt_builder, topic_parser
from .transparent_fusion import transparent_fusion
from .local_llm_client import local_llm_client

router = APIRouter(prefix="/ai", tags=["AI"])

class AskRequest(BaseModel):
    prompt: str

@router.get("/test_connection")
async def test_anythingllm_connection():
    """æ¸¬è©¦AnythingLLMé€£æ¥çš„ç«¯é»"""
    return await ai_client.test_connection()

class TestWorkspaceRequest(BaseModel):
    room_code: str
    room_title: str

@router.post("/test_create_workspace")
async def test_create_workspace(req: TestWorkspaceRequest):
    """æ¸¬è©¦å‰µå»ºå·¥ä½œå€çš„ç«¯é»"""
    try:
        workspace_slug = await ai_client.ensure_workspace_exists(req.room_code, req.room_title)
        return {
            "status": "success",
            "message": f"å·¥ä½œå€æ“ä½œæˆåŠŸ",
            "workspace_slug": workspace_slug,
            "room_code": req.room_code,
            "room_title": req.room_title
        }
    except Exception as e:
        return {"status": "error", "message": f"å‰µå»ºå·¥ä½œå€å¤±æ•—: {str(e)}"}

@router.post("/ask")
async def ask_ai(req: AskRequest):
    """AIå•ç­” - é€æ˜èåˆç³»çµ±ï¼ˆNPU+CPUä¸¦è¡Œ->NPUèåˆï¼‰"""
    try:
        # ä½¿ç”¨é€æ˜èåˆå¼•æ“ï¼ŒèƒŒæ™¯ä¸¦è¡ŒNPU+CPUï¼Œæœ€çµ‚NPUèåˆæ¨ç†
        answer = await transparent_fusion.process_request(req.prompt, task_type="chat")
        return {"answer": answer}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AI è™•ç†å¤±æ•—: {str(e)}")

# å®šç¾©ç”¨æ–¼ AI ç¸½çµçš„è«‹æ±‚æ¨¡å‹
class SummaryRequest(BaseModel):
    room: str  # æœƒè­°å®¤ä»£ç¢¼
    topic: str # è¦ç¸½çµçš„ä¸»é¡Œ

@router.post("/summary")
async def summary_ai(req: SummaryRequest):
    """
    å°æŒ‡å®šæœƒè­°å®¤çš„ç‰¹å®šä¸»é¡Œé€²è¡Œ AI ç¸½çµ

    [POST] /ai/summary

    åƒæ•¸ï¼š
    - room (str): æœƒè­°å®¤ä»£ç¢¼
    - topic (str): è¦é€²è¡Œç¸½çµçš„ä¸»é¡Œåç¨±

    å›å‚³ï¼š
    - summary (str): AI ç”Ÿæˆçš„ç¸½çµæ–‡å­—
    """
    # æª¢æŸ¥æœƒè­°å®¤æ˜¯å¦å­˜åœ¨
    if req.room not in ROOMS:
        return {"summary": "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æœƒè­°å®¤ã€‚"}

    # çµ„åˆä¸»é¡Œ ID ä¸¦æª¢æŸ¥ä¸»é¡Œæ˜¯å¦å­˜åœ¨
    topic_id = f"{req.room}_{req.topic}"
    if topic_id not in topics:
        return {"summary": "éŒ¯èª¤ï¼šåœ¨è©²æœƒè­°å®¤ä¸­æ‰¾ä¸åˆ°æŒ‡å®šçš„ä¸»é¡Œã€‚"}

    # ä½¿ç”¨prompt_builderçš„æ–¹æ³•ä¾†å»ºç«‹ prompt
    prompt = prompt_builder.build_summary_prompt(req.room, req.topic)
    
    if prompt.startswith("éŒ¯èª¤"):
        return {"summary": prompt}

    # ä½¿ç”¨é€æ˜èåˆç³»çµ±ç”Ÿæˆæ‘˜è¦
    try:
        # ç²å–æœƒè­°å®¤è³‡è¨Š
        room_data = ROOMS[req.room]
        room_title = room_data.get('title', f'æœƒè­°å®¤-{req.room}')
        
        # å„ªå…ˆä½¿ç”¨æœƒè­°å‰µå»ºæ™‚ä¿å­˜çš„workspaceï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º
        workspace_slug = room_data.get('workspace_slug')
        if not workspace_slug:
            print(f"âš ï¸ æœƒè­° {req.room} æ²’æœ‰é è¨­workspaceï¼Œæ­£åœ¨å‰µå»º...")
            workspace_slug = await ai_client.ensure_workspace_exists(req.room, room_title)
            # æ›´æ–°æœƒè­°æ•¸æ“š
            ROOMS[req.room]['workspace_slug'] = workspace_slug
            workspace_info = await ai_client.get_workspace_info(workspace_slug)
            if workspace_info and "id" in workspace_info:
                ROOMS[req.room]['workspace_id'] = workspace_info["id"]
        else:
            print(f"âœ… ä½¿ç”¨æœƒè­°å°ˆå±¬workspace: {workspace_slug}")
        
        # ä½¿ç”¨é€æ˜èåˆç³»çµ±ï¼šNPU+CPUä¸¦è¡Œ->NPUèåˆï¼Œæå‡æ‘˜è¦è³ªé‡
        summary_text = await transparent_fusion.process_request(
            prompt, workspace_slug, task_type="summary"
        )
        return {"summary": summary_text}
    except HTTPException:
        raise
    except Exception as e:
        return {"summary": f"AI ç¸½çµç”Ÿæˆå¤±æ•—: {str(e)}"}

async def _generate_topics_from_title(meeting_title: str, topic_count: int, workspace_slug: str = None) -> List[str]:
    """
    æ ¹æ“šæœƒè­°æ¨™é¡Œç”Ÿæˆä¸»é¡Œçš„æ ¸å¿ƒé‚è¼¯ã€‚
    é€™æ˜¯ä¸€å€‹å…§éƒ¨å‡½å¼ï¼Œæ—¨åœ¨è¢«å…¶ä»– API ç«¯é»èª¿ç”¨ã€‚
    """
    topic_count = max(1, min(10, topic_count))
    meeting_title = meeting_title.strip()

    if not meeting_title:
        return ["éŒ¯èª¤ï¼šæœƒè­°åç¨±ä¸å¯ç‚ºç©ºã€‚"]

    prompt = prompt_builder.build_topics_generation_prompt(meeting_title, topic_count)
    
    if prompt.startswith("éŒ¯èª¤"):
        return [prompt]

    try:
        # ä½¿ç”¨é€æ˜èåˆç³»çµ±ç”Ÿæˆä¸»é¡Œï¼šNPU+CPUä¸¦è¡Œ->NPUèåˆ
        raw_text = await transparent_fusion.process_request(prompt, workspace_slug, task_type="topic_generation")
        return topic_parser.parse_topics_from_response(raw_text, topic_count)

    except Exception as e:
        print(f"Error calling LLM for topic generation: {e}")
        return [f"AIæœå‹™æš«æ™‚ç„¡æ³•é€£ç·šï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"]


class GenerateTopicsRequest(BaseModel):
    """ç”¨æ–¼ AI ç”Ÿæˆä¸»é¡Œè«‹æ±‚çš„æ¨¡å‹"""
    meeting_title: str
    topic_count: int
    room_code: Optional[str] = None  # å¯é¸çš„æœƒè­°ä»£ç¢¼ï¼Œå¦‚æœæä¾›å‰‡ä½¿ç”¨æœƒè­°å°ˆå±¬workspace

@router.post("/generate_topics")
async def generate_ai_topics(req: GenerateTopicsRequest):
    """
    æ ¹æ“šæœƒè­°åç¨±å’ŒæŒ‡å®šæ•¸é‡ï¼Œä½¿ç”¨ AI ç”Ÿæˆè­°ç¨‹ä¸»é¡Œã€‚

    [POST] /ai/generate_topics

    åƒæ•¸ï¼š
    - meeting_title (str): æœƒè­°çš„æ¨™é¡Œæˆ–ä¸»è¦ç›®çš„ã€‚
    - topic_count (int): å¸Œæœ›ç”Ÿæˆçš„ä¸»é¡Œæ•¸é‡ã€‚

    å›å‚³ï¼š
    - topics (list[str]): ä¸€å€‹åŒ…å« AI ç”Ÿæˆçš„ä¸»é¡Œå­—ä¸²çš„åˆ—è¡¨ã€‚
    """
    # --- 1. åŸºæœ¬é©—è­‰ ---
    # ç¢ºä¿ä¸»é¡Œæ•¸é‡åœ¨ä¸€å€‹åˆç†çš„ç¯„åœå…§
    topic_count = max(1, min(10, req.topic_count)) 
    meeting_title = req.meeting_title.strip()

    if not meeting_title:
        return {"topics": ["éŒ¯èª¤ï¼šæœƒè­°åç¨±ä¸å¯ç‚ºç©ºã€‚"]}

    # å‘¼å« AnythingLLM API
    try:
        # å„ªå…ˆä½¿ç”¨æœƒè­°å°ˆå±¬workspaceï¼Œå¦‚æœæ²’æœ‰æä¾›room_codeå‰‡å‰µå»ºè‡¨æ™‚workspace
        if req.room_code and req.room_code in ROOMS:
            # ä½¿ç”¨çœŸå¯¦æœƒè­°çš„å°ˆå±¬workspace
            room_data = ROOMS[req.room_code]
            workspace_slug = room_data.get('workspace_slug')
            if not workspace_slug:
                print(f"âš ï¸ æœƒè­° {req.room_code} æ²’æœ‰é è¨­workspaceï¼Œæ­£åœ¨å‰µå»º...")
                workspace_slug = await ai_client.ensure_workspace_exists(req.room_code, meeting_title)
                # æ›´æ–°æœƒè­°æ•¸æ“š
                ROOMS[req.room_code]['workspace_slug'] = workspace_slug
                workspace_info = await ai_client.get_workspace_info(workspace_slug)
                if workspace_info and "id" in workspace_info:
                    ROOMS[req.room_code]['workspace_id'] = workspace_info["id"]
            else:
                print(f"âœ… ä½¿ç”¨æœƒè­°å°ˆå±¬workspace: {workspace_slug}")
        else:
            # å‚™é¸æ–¹æ¡ˆï¼šç‚ºç¨ç«‹çš„ä¸»é¡Œç”Ÿæˆå‰µå»ºè‡¨æ™‚workspace
            print(f"ğŸ“ ç‚ºç¨ç«‹ä¸»é¡Œç”Ÿæˆå‰µå»ºè‡¨æ™‚workspace...")
            temp_room_code = f"topics-{hash(meeting_title) % 10000}"
            workspace_slug = await ai_client.ensure_workspace_exists(temp_room_code, meeting_title)
        
        generated_topics = await _generate_topics_from_title(meeting_title, topic_count, workspace_slug)
        return {"topics": generated_topics}

    except Exception as e:
        # è™•ç†å‘¼å« AI æ™‚å¯èƒ½ç™¼ç”Ÿçš„ä»»ä½•éŒ¯èª¤
        print(f"Error calling LLM for topic generation: {e}")
        return {"topics": [f"AI æœå‹™æš«æ™‚ç„¡æ³•é€£ç·šï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"]}

# æ–°å¢çš„è«‹æ±‚æ¨¡å‹ï¼Œç”¨æ–¼ generate_single_topic ç«¯é»
class GenerateSingleTopicRequest(BaseModel):
    room: str
    custom_prompt: str

@router.post("/generate_single_topic")
async def generate_single_topic(req: GenerateSingleTopicRequest):
    """
    æ ¹æ“šæœƒè­°å®¤å’Œè‡ªè¨‚æç¤ºï¼Œä½¿ç”¨ AI ç”Ÿæˆå–®ä¸€è­°ç¨‹ä¸»é¡Œã€‚

    [POST] /ai/generate_single_topic

    åƒæ•¸ï¼š
    - room (str): æœƒè­°å®¤ä»£ç¢¼
    - custom_prompt (str): è‡ªè¨‚çš„æç¤ºèªå¥ï¼Œç”¨æ–¼å¼•å° AI ç”Ÿæˆä¸»é¡Œ

    å›å‚³ï¼š
    - topic (str): AI ç”Ÿæˆçš„ä¸»é¡Œå­—ä¸²
    """
    # æª¢æŸ¥æœƒè­°å®¤æ˜¯å¦å­˜åœ¨
    if req.room not in ROOMS:
        return {"topic": "éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æŒ‡å®šçš„æœƒè­°å®¤ã€‚"}

    room_data = ROOMS[req.room]

    # ä½¿ç”¨prompt_builderçš„æ–¹æ³•ä¾†å»ºç«‹ prompt
    prompt = prompt_builder.build_single_topic_generation_prompt(req.room, req.custom_prompt)
    
    if prompt.startswith("éŒ¯èª¤"):
        return {"topic": prompt}

    # ä½¿ç”¨é€æ˜èåˆç³»çµ±ç”Ÿæˆå–®ä¸€ä¸»é¡Œ
    try:
        # ç²å–æœƒè­°å®¤è³‡è¨Š
        room_data = ROOMS[req.room]
        room_title = room_data.get('title', f'æœƒè­°å®¤-{req.room}')
        
        # å„ªå…ˆä½¿ç”¨æœƒè­°å‰µå»ºæ™‚ä¿å­˜çš„workspaceï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º
        workspace_slug = room_data.get('workspace_slug')
        if not workspace_slug:
            print(f"âš ï¸ æœƒè­° {req.room} æ²’æœ‰é è¨­workspaceï¼Œæ­£åœ¨å‰µå»º...")
            workspace_slug = await ai_client.ensure_workspace_exists(req.room, room_title)
            # æ›´æ–°æœƒè­°æ•¸æ“š
            ROOMS[req.room]['workspace_slug'] = workspace_slug
            workspace_info = await ai_client.get_workspace_info(workspace_slug)
            if workspace_info and "id" in workspace_info:
                ROOMS[req.room]['workspace_id'] = workspace_info["id"]
        else:
            print(f"âœ… ä½¿ç”¨æœƒè­°å°ˆå±¬workspace: {workspace_slug}")
        
        # ä½¿ç”¨é€æ˜èåˆç³»çµ±ï¼šNPU+CPUä¸¦è¡Œ->NPUèåˆ
        topic = await transparent_fusion.process_request(
            prompt, workspace_slug, task_type="single_topic"
        )
        return {"topic": topic.strip()}
    except HTTPException:
        raise
    except Exception as e:
        return {"topic": f"AI ä¸»é¡Œç”Ÿæˆå¤±æ•—: {str(e)}"}

# é€æ˜èåˆç³»çµ±ç®¡ç†ç«¯é»
@router.get("/fusion/status")
async def get_fusion_status():
    """ç²å–é€æ˜èåˆç³»çµ±ç‹€æ…‹"""
    stats = transparent_fusion.get_stats()
    health = transparent_fusion.get_health_status()
    return {
        "fusion_enabled": transparent_fusion.is_fusion_enabled(),
        "status": "é‹è¡Œä¸­" if transparent_fusion.is_fusion_enabled() else "å·²ç¦ç”¨",
        "statistics": stats,
        "health": health,
        "message": "Snapdragon Elite Xé€æ˜èåˆï¼šNPU+CPUä¸¦è¡Œ->NPUèåˆï¼Œç”¨æˆ¶ç„¡æ„ŸçŸ¥è³ªé‡æå‡"
    }

@router.post("/fusion/toggle")
async def toggle_fusion(enable: bool = True):
    """åˆ‡æ›èåˆç³»çµ±ç‹€æ…‹ï¼ˆç®¡ç†å“¡åŠŸèƒ½ï¼‰"""
    if enable:
        transparent_fusion.enable_fusion()
        message = "é€æ˜èåˆç³»çµ±å·²å•Ÿç”¨ - Snapdragon Elite X: NPU+CPUä¸¦è¡Œ->NPUèåˆæ¨ç†"
    else:
        transparent_fusion.disable_fusion()
        message = "èåˆç³»çµ±å·²ç¦ç”¨ - å›é€€åˆ°å–®ä¸€NPUæ¨¡å¼"
    
    return {
        "status": "success",
        "fusion_enabled": transparent_fusion.is_fusion_enabled(),
        "message": message,
        "note": "APIæ ¼å¼ä¿æŒå®Œå…¨ä¸è®Šï¼Œç”¨æˆ¶æ„ŸçŸ¥ä¸åˆ°ä»»ä½•å·®ç•°",
        "platform": "Snapdragon Elite Xå„ªåŒ–"
    }

# CPUæ¨¡å‹ç®¡ç†ç«¯é»
@router.post("/cpu_model/load")
async def load_cpu_model(model_name: str = "phi3-mini"):
    """åŠ è¼‰CPUæ¨¡å‹ï¼ˆSnapdragon Elite Xå„ªåŒ–ï¼‰"""
    try:
        success = await local_llm_client.load_model(model_name=model_name)
        if success:
            model_info = local_llm_client.get_model_info()
            return {
                "status": "success",
                "message": f"CPUæ¨¡å‹ {model_name} åŠ è¼‰æˆåŠŸ",
                "model_info": model_info,
                "platform_optimization": "Snapdragon Elite X - 8æ ¸å¿ƒCPUå„ªåŒ–"
            }
        else:
            return {
                "status": "error",
                "message": f"CPUæ¨¡å‹ {model_name} åŠ è¼‰å¤±æ•—"
            }
    except Exception as e:
        return {
            "status": "error", 
            "message": f"CPUæ¨¡å‹åŠ è¼‰éŒ¯èª¤: {str(e)}"
        }

@router.post("/cpu_model/download")
async def download_cpu_model(model_name: str = "phi3-mini"):
    """å¾Hugging Faceä¸‹è¼‰CPUæ¨¡å‹"""
    try:
        model_path = await local_llm_client.download_model(model_name)
        return {
            "status": "success",
            "message": f"æ¨¡å‹ {model_name} ä¸‹è¼‰å®Œæˆ",
            "model_path": model_path,
            "supported_models": list(local_llm_client.supported_models.keys())
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"æ¨¡å‹ä¸‹è¼‰å¤±æ•—: {str(e)}"
        }

@router.get("/cpu_model/info")
async def get_cpu_model_info():
    """ç²å–CPUæ¨¡å‹ä¿¡æ¯"""
    return {
        "model_info": local_llm_client.get_model_info(),
        "supported_models": local_llm_client.supported_models,
        "platform": "Snapdragon Elite X"
    }

@router.post("/cpu_model/test")
async def test_cpu_model():
    """æ¸¬è©¦CPUæ¨¡å‹æ¨ç†åŠŸèƒ½"""
    try:
        result = await local_llm_client.test_generation()
        return result
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

# èåˆç³»çµ±é«˜ç´šç®¡ç†ç«¯é»
@router.post("/fusion/cache/clear")
async def clear_fusion_cache():
    """æ¸…ç©ºèåˆç³»çµ±ç·©å­˜"""
    transparent_fusion.clear_cache()
    return {
        "status": "success",
        "message": "èåˆç³»çµ±ç·©å­˜å·²æ¸…ç©º"
    }

@router.post("/fusion/stats/reset")
async def reset_fusion_stats():
    """é‡ç½®èåˆç³»çµ±çµ±è¨ˆ"""
    transparent_fusion.reset_stats()
    return {
        "status": "success",
        "message": "èåˆç³»çµ±çµ±è¨ˆå·²é‡ç½®"
    }

@router.get("/fusion/health")
async def get_fusion_health():
    """ç²å–èåˆç³»çµ±å¥åº·ç‹€æ³"""
    health = transparent_fusion.get_health_status()
    return {
        "system": "é€æ˜èåˆå¼•æ“",
        "platform": "Snapdragon Elite X",
        "architecture": "NPU+CPUä¸¦è¡Œ -> NPUèåˆæ¨ç†",
        "health": health,
        "timestamp": time.time()
    }