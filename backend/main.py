# backend/main.py
from fastapi import FastAPI
from backend.api import ai_api
from fastapi.middleware.cors import CORSMiddleware
from backend.api import participants_api
from backend.api import network_api
from backend.api import mindmap_api as mindmap_api
import asyncio
import logging

app = FastAPI(title="MBBuddy API")

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æˆ– ["http://localhost:5173", "http://192.168.0.214:5173"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(participants_api.router)
app.include_router(ai_api.router)
app.include_router(network_api.router)
app.include_router(mindmap_api.router)

@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚é è¼‰å…¥AIæ¨¡å‹"""
    logger.info("ğŸš€ SyncAI å¾Œç«¯æœå‹™å•Ÿå‹•ä¸­...")
    
    # é è¼‰å…¥ CPU LLM æ¨¡å‹
    try:
        from backend.api.local_llm_client import local_llm_client
        logger.info("ğŸ“¥ é–‹å§‹é è¼‰å…¥ CPU LLM æ¨¡å‹...")
        
        # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_dir = local_llm_client.models_dir / "qwen2-1.5b"
        if model_dir.exists():
            model_files = list(model_dir.glob("*.gguf"))
            if model_files:
                model_path = str(model_files[0])
                logger.info(f"ğŸ“ æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
                
                # é è¼‰å…¥æ¨¡å‹
                success = await local_llm_client.load_model(model_path, "qwen2-1.5b")
                if success:
                    logger.info("âœ… CPU LLM æ¨¡å‹é è¼‰å…¥æˆåŠŸ")
                else:
                    logger.warning("âš ï¸ CPU LLM æ¨¡å‹é è¼‰å…¥å¤±æ•—")
            else:
                logger.warning("âš ï¸ æ‰¾ä¸åˆ° .gguf æ¨¡å‹æ–‡ä»¶")
        else:
            logger.warning("âš ï¸ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨ï¼Œå°‡åœ¨é¦–æ¬¡èª¿ç”¨æ™‚ä¸‹è¼‰")
            
    except Exception as e:
        logger.error(f"âŒ é è¼‰å…¥ CPU LLM æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # æ¸¬è©¦ AnythingLLM é€£æ¥
    try:
        from backend.api.ai_client import ai_client
        logger.info("ğŸ”— æ¸¬è©¦ AnythingLLM é€£æ¥...")
        
        # ç°¡å–®çš„é€£æ¥æ¸¬è©¦
        test_result = await ai_client.test_connection()
        if test_result:
            logger.info("âœ… AnythingLLM é€£æ¥æ­£å¸¸")
        else:
            logger.warning("âš ï¸ AnythingLLM é€£æ¥æ¸¬è©¦å¤±æ•—")
            
    except Exception as e:
        logger.error(f"âŒ AnythingLLM é€£æ¥æ¸¬è©¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    logger.info("ğŸ‰ SyncAI å¾Œç«¯æœå‹™å•Ÿå‹•å®Œæˆï¼")

@app.on_event("shutdown")
async def shutdown_event():
    """æ‡‰ç”¨é—œé–‰æ™‚æ¸…ç†è³‡æº"""
    logger.info("ğŸ›‘ SyncAI å¾Œç«¯æœå‹™æ­£åœ¨é—œé–‰...")
    
    try:
        from backend.api.local_llm_client import local_llm_client
        if local_llm_client.is_model_loaded():
            local_llm_client.unload_model()
            logger.info("âœ… CPU LLM æ¨¡å‹å·²å¸è¼‰")
    except Exception as e:
        logger.error(f"âŒ å¸è¼‰æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    logger.info("ğŸ‘‹ SyncAI å¾Œç«¯æœå‹™å·²é—œé–‰")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("backend.main:app", host="0.0.0.0", port=8001, reload=True)
